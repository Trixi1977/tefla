{
    "docs": [
        {
            "location": "/", 
            "text": "Tefla: Deep Learning library, a Higher level API for TensorFlow\n\n\nTefla is built on top of Tensorflow. It provides higher level access to tensorflow's features.  \n\n\nTefla features:\n\n\n    . Support for data-sets, data-augmentation\n\n    . easy to define complex deep models\n\n    . single and multi GPU training\n\n    . various prediction fnctions including ensembling of models\n\n    . different metrics for performance measurement\\\n\n    . custom losses\n\n    . learning rate schedules, polynomial, step, validation_loss based\n\n\n\nTensorFlow Installation\n\n\nTefla requires Tensorflow(version \n=r0.12)\n\n\nTefla Installation\n\n\nfor current version installation:\n\n\npip install git+https://github.com/n3011/tefla.git\n\n\n\n\nExamples\n\n\nMnist example gives a overview about Tefla usages\n\n\ndef model(is_training, reuse):\n    common_args = common_layer_args(is_training, reuse)\n    conv_args = make_args(batch_norm=True, activation=prelu, **common_args)\n    fc_args = make_args(activation=prelu, **common_args)\n    logit_args = make_args(activation=None, **common_args)\n\n    x = input((None, height, width, 1), **common_args)\n    x = conv2d(x, 32, name='conv1_1', **conv_args)\n    x = conv2d(x, 32, name='conv1_2', **conv_args)\n    x = max_pool(x, name='pool1', **common_args)\n    x = dropout(x, drop_p=0.25, name='dropout1', **common_args)\n    x = fully_connected(x, n_output=128, name='fc1', **fc_args)\n    x = dropout(x, drop_p=0.5, name='dropout2', **common_args)\n    logits = fully_connected(x, n_output=10, name=\nlogits\n, **logit_args)\n    predictions = softmax(logits, name='predictions', **common_args)\n\n    return end_points(is_training)\n\ntraining_cnf = {\n    'classification': True,\n    'validation_scores': [('validation accuracy', util.accuracy_wrapper), ('validation kappa', util.kappa_wrapper)],\n    'num_epochs': 50,\n    'lr_policy': StepDecayPolicy(\n        schedule={\n            0: 0.01,\n            30: 0.001,\n        }\n    )\n}\nutil.init_logging('train.log', file_log_level=logging.INFO, console_log_level=logging.INFO)\n\ntrainer = SupervisedTrainer(model, training_cnf, classification=training_cnf['classification'])\ntrainer.fit(data_set, weights_from=None, start_epoch=1, verbose=1, summary_every=10)", 
            "title": "Home"
        }, 
        {
            "location": "/#tefla-deep-learning-library-a-higher-level-api-for-tensorflow", 
            "text": "Tefla is built on top of Tensorflow. It provides higher level access to tensorflow's features.    Tefla features:      . Support for data-sets, data-augmentation\n\n    . easy to define complex deep models\n\n    . single and multi GPU training\n\n    . various prediction fnctions including ensembling of models\n\n    . different metrics for performance measurement\\\n\n    . custom losses\n\n    . learning rate schedules, polynomial, step, validation_loss based  TensorFlow Installation  Tefla requires Tensorflow(version  =r0.12)  Tefla Installation  for current version installation:  pip install git+https://github.com/n3011/tefla.git", 
            "title": "Tefla: Deep Learning library, a Higher level API for TensorFlow"
        }, 
        {
            "location": "/#examples", 
            "text": "Mnist example gives a overview about Tefla usages  def model(is_training, reuse):\n    common_args = common_layer_args(is_training, reuse)\n    conv_args = make_args(batch_norm=True, activation=prelu, **common_args)\n    fc_args = make_args(activation=prelu, **common_args)\n    logit_args = make_args(activation=None, **common_args)\n\n    x = input((None, height, width, 1), **common_args)\n    x = conv2d(x, 32, name='conv1_1', **conv_args)\n    x = conv2d(x, 32, name='conv1_2', **conv_args)\n    x = max_pool(x, name='pool1', **common_args)\n    x = dropout(x, drop_p=0.25, name='dropout1', **common_args)\n    x = fully_connected(x, n_output=128, name='fc1', **fc_args)\n    x = dropout(x, drop_p=0.5, name='dropout2', **common_args)\n    logits = fully_connected(x, n_output=10, name= logits , **logit_args)\n    predictions = softmax(logits, name='predictions', **common_args)\n\n    return end_points(is_training)\n\ntraining_cnf = {\n    'classification': True,\n    'validation_scores': [('validation accuracy', util.accuracy_wrapper), ('validation kappa', util.kappa_wrapper)],\n    'num_epochs': 50,\n    'lr_policy': StepDecayPolicy(\n        schedule={\n            0: 0.01,\n            30: 0.001,\n        }\n    )\n}\nutil.init_logging('train.log', file_log_level=logging.INFO, console_log_level=logging.INFO)\n\ntrainer = SupervisedTrainer(model, training_cnf, classification=training_cnf['classification'])\ntrainer.fit(data_set, weights_from=None, start_epoch=1, verbose=1, summary_every=10)", 
            "title": "Examples"
        }, 
        {
            "location": "/", 
            "text": "Tefla: Deep Learning library, a Higher level API for TensorFlow\n\n\nTefla is built on top of Tensorflow. It provides higher level access to tensorflow's features.  \n\n\nTefla features:\n\n\n    . Support for data-sets, data-augmentation\n\n    . easy to define complex deep models\n\n    . single and multi GPU training\n\n    . various prediction fnctions including ensembling of models\n\n    . different metrics for performance measurement\\\n\n    . custom losses\n\n    . learning rate schedules, polynomial, step, validation_loss based\n\n\n\nTensorFlow Installation\n\n\nTefla requires Tensorflow(version \n=r0.12)\n\n\nTefla Installation\n\n\nfor current version installation:\n\n\npip install git+https://github.com/n3011/tefla.git\n\n\n\n\nExamples\n\n\nMnist example gives a overview about Tefla usages\n\n\ndef model(is_training, reuse):\n    common_args = common_layer_args(is_training, reuse)\n    conv_args = make_args(batch_norm=True, activation=prelu, **common_args)\n    fc_args = make_args(activation=prelu, **common_args)\n    logit_args = make_args(activation=None, **common_args)\n\n    x = input((None, height, width, 1), **common_args)\n    x = conv2d(x, 32, name='conv1_1', **conv_args)\n    x = conv2d(x, 32, name='conv1_2', **conv_args)\n    x = max_pool(x, name='pool1', **common_args)\n    x = dropout(x, drop_p=0.25, name='dropout1', **common_args)\n    x = fully_connected(x, n_output=128, name='fc1', **fc_args)\n    x = dropout(x, drop_p=0.5, name='dropout2', **common_args)\n    logits = fully_connected(x, n_output=10, name=\nlogits\n, **logit_args)\n    predictions = softmax(logits, name='predictions', **common_args)\n\n    return end_points(is_training)\n\ntraining_cnf = {\n    'classification': True,\n    'validation_scores': [('validation accuracy', util.accuracy_wrapper), ('validation kappa', util.kappa_wrapper)],\n    'num_epochs': 50,\n    'lr_policy': StepDecayPolicy(\n        schedule={\n            0: 0.01,\n            30: 0.001,\n        }\n    )\n}\nutil.init_logging('train.log', file_log_level=logging.INFO, console_log_level=logging.INFO)\n\ntrainer = SupervisedTrainer(model, training_cnf, classification=training_cnf['classification'])\ntrainer.fit(data_set, weights_from=None, start_epoch=1, verbose=1, summary_every=10)", 
            "title": "Index"
        }, 
        {
            "location": "/#tefla-deep-learning-library-a-higher-level-api-for-tensorflow", 
            "text": "Tefla is built on top of Tensorflow. It provides higher level access to tensorflow's features.    Tefla features:      . Support for data-sets, data-augmentation\n\n    . easy to define complex deep models\n\n    . single and multi GPU training\n\n    . various prediction fnctions including ensembling of models\n\n    . different metrics for performance measurement\\\n\n    . custom losses\n\n    . learning rate schedules, polynomial, step, validation_loss based  TensorFlow Installation  Tefla requires Tensorflow(version  =r0.12)  Tefla Installation  for current version installation:  pip install git+https://github.com/n3011/tefla.git", 
            "title": "Tefla: Deep Learning library, a Higher level API for TensorFlow"
        }, 
        {
            "location": "/#examples", 
            "text": "Mnist example gives a overview about Tefla usages  def model(is_training, reuse):\n    common_args = common_layer_args(is_training, reuse)\n    conv_args = make_args(batch_norm=True, activation=prelu, **common_args)\n    fc_args = make_args(activation=prelu, **common_args)\n    logit_args = make_args(activation=None, **common_args)\n\n    x = input((None, height, width, 1), **common_args)\n    x = conv2d(x, 32, name='conv1_1', **conv_args)\n    x = conv2d(x, 32, name='conv1_2', **conv_args)\n    x = max_pool(x, name='pool1', **common_args)\n    x = dropout(x, drop_p=0.25, name='dropout1', **common_args)\n    x = fully_connected(x, n_output=128, name='fc1', **fc_args)\n    x = dropout(x, drop_p=0.5, name='dropout2', **common_args)\n    logits = fully_connected(x, n_output=10, name= logits , **logit_args)\n    predictions = softmax(logits, name='predictions', **common_args)\n\n    return end_points(is_training)\n\ntraining_cnf = {\n    'classification': True,\n    'validation_scores': [('validation accuracy', util.accuracy_wrapper), ('validation kappa', util.kappa_wrapper)],\n    'num_epochs': 50,\n    'lr_policy': StepDecayPolicy(\n        schedule={\n            0: 0.01,\n            30: 0.001,\n        }\n    )\n}\nutil.init_logging('train.log', file_log_level=logging.INFO, console_log_level=logging.INFO)\n\ntrainer = SupervisedTrainer(model, training_cnf, classification=training_cnf['classification'])\ntrainer.fit(data_set, weights_from=None, start_epoch=1, verbose=1, summary_every=10)", 
            "title": "Examples"
        }, 
        {
            "location": "/core/layers/", 
            "text": "Define input layer\n\n\ntefla.core.layers.input\n  (shape,  name='inputs',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nshape\n: A \nTensor\n, define the input shape\ne.g. for image input [batch_size, height, width, depth]\n\n\nname\n: A optional score/name for this op\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA placeholder for the input\n\n\n\n\nAdds a fully connected layer\n\n\ntefla.core.layers.fully_connected\n  (x,  n_output,  is_training,  reuse,  trainable=True,  w_init=\n,  b_init=0.0,  w_regularizer=\n,  name='fc',  batch_norm=None,  batch_norm_args=None,  activation=None,  outputs_collections=None,  use_bias=True)\n\n\nfully_connected\n creates a variable called \nweights\n, representing a fully\nconnected weight matrix, which is multiplied by the \nx\n to produce a\n\nTensor\n of hidden units. If a \nbatch_norm\n is provided (such as\n\nbatch_norm\n), it is then applied. Otherwise, if \nbatch_norm\n is\nNone and a \nb_init\n and \nuse_bias\n is provided then a \nbiases\n variable would be\ncreated and added the hidden units. Finally, if \nactivation\n is not \nNone\n,\nit is applied to the hidden units as well.\nNote: that if \nx\n have a rank greater than 2, then \nx\n is flattened\nprior to the initial matrix multiply by \nweights\n.\n\n\nArgs\n\n\n\n\n\nx\n: A \nTensor\n of with at least rank 2 and value for the last dimension,\ni.e. \n[batch_size, depth]\n, \n[None, None, None, channels]\n.\n\n\nis_training\n: Bool, training or testing\n\n\nn_output\n: Integer or long, the number of output units in the layer.\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\n\n\nactivation\n: activation function, set to None to skip it and maintain\na linear activation.\n\n\nbatch_norm\n: normalization function to use. If\n -\nbatch_norm\n is \nTrue\n then google original implementation is used and\nif another function is provided then it is applied.\ndefault set to None for no normalizer function\n\n\nbatch_norm_args\n: normalization function parameters.\n\n\nw_init\n: An initializer for the weights.\n\n\nw_regularizer\n: Optional regularizer for the weights.\n\n\nb_init\n: An initializer for the biases. If None skip biases.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\ntrainable\n: If \nTrue\n also add variables to the graph collection\n\nGraphKeys.TRAINABLE_VARIABLES\n (see tf.Variable).\n\n\nname\n: Optional name or scope for variable_scope/name_scope.\n\n\nuse_bias\n: Whether to add bias or not\n\n\n\n\nReturns\n\n\n\nThe 2-D \nTensor\n variable representing the result of the series of operations.\ne.g: 2-D \nTensor\n [batch, n_output].\n\n\n\n\nAdds a 2D convolutional layer\n\n\ntefla.core.layers.conv2d\n  (x,  n_output_channels,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  stride=  (1,  1),  padding='SAME',  w_init=\n,  b_init=0.0,  w_regularizer=\n,  untie_biases=False,  name='conv2d',  batch_norm=None,  batch_norm_args=None,  activation=None,  use_bias=True,  outputs_collections=None)\n\n\nconvolutional layer\n creates a variable called \nweights\n, representing a conv\nweight matrix, which is multiplied by the \nx\n to produce a\n\nTensor\n of hidden units. If a \nbatch_norm\n is provided (such as\n\nbatch_norm\n), it is then applied. Otherwise, if \nbatch_norm\n is\nNone and a \nb_init\n and \nuse_bias\n is provided then a \nbiases\n variable would be\ncreated and added the hidden units. Finally, if \nactivation\n is not \nNone\n,\nit is applied to the hidden units as well.\nNote: that if \nx\n have a rank 4\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D \nTensor\n of with at least rank 2 and value for the last dimension,\ni.e. \n[batch_size, in_height, in_width, depth]\n,\n\n\nis_training\n: Bool, training or testing\n\n\nn_output\n: Integer or long, the number of output units in the layer.\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\n\n\n\n\nReturns\n\n\n\nThe 4-D \nTensor\n variable representing the result of the series of operations.\ne.g.: 4-D \nTensor\n [batch, new_height, new_width, n_output].\n\n\n\n\nAdds a 2D dilated convolutional layer, also known as convolution with holes or atrous convolution\n\n\ntefla.core.layers.dilated_conv2d\n  (x,  n_output_channels,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  dilation=1,  padding='SAME',  w_init=\n,  b_init=0.0,  w_regularizer=\n,  untie_biases=False,  name='dilated_conv2d',  batch_norm=None,  batch_norm_args=None,  activation=None,  use_bias=True,  outputs_collections=None)\n\nIf the rate parameter is equal to one, it performs regular 2-D convolution. If the rate parameter\nis greater than one, it performs convolution with holes, sampling the input\nvalues every rate pixels in the height and width dimensions.\n\nconvolutional layer\n creates a variable called \nweights\n, representing a conv\nweight matrix, which is multiplied by the \nx\n to produce a\n\nTensor\n of hidden units. If a \nbatch_norm\n is provided (such as\n\nbatch_norm\n), it is then applied. Otherwise, if \nbatch_norm\n is\nNone and a \nb_init\n and \nuse_bias\n is provided then a \nbiases\n variable would be\ncreated and added the hidden units. Finally, if \nactivation\n is not \nNone\n,\nit is applied to the hidden units as well.\nNote: that if \nx\n have a rank 4\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D \nTensor\n of with rank 4 and value for the last dimension,\ni.e. \n[batch_size, in_height, in_width, depth]\n,\n\n\nis_training\n: Bool, training or testing\n\n\nn_output\n: Integer or long, the number of output units in the layer.\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\n\n\n\n\nReturns\n\n\n\nThe 4-D \nTensor\n variable representing the result of the series of operations.\ne.g.: 4-D \nTensor\n [batch, new_height, new_width, n_output].\n\n\n\n\nAdds a 2D seperable convolutional layer\n\n\ntefla.core.layers.separable_conv2d\n  (x,  n_output_channels,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  stride=  (1,  1),  depth_multiplier=8,  padding='SAME',  w_init=\n,  b_init=0.0,  w_regularizer=\n,  untie_biases=False,  name='separable_conv2d',  batch_norm=None,  batch_norm_args=None,  activation=None,  use_bias=True,  outputs_collections=None)\n\nPerforms a depthwise convolution that acts separately on channels followed by\na pointwise convolution that mixes channels. Note that this is separability between\ndimensions [1, 2] and 3, not spatial separability between dimensions 1 and 2.\n\nconvolutional layer\n creates two variable called \ndepthwise_W\n and \npointwise_W\n,\n\ndepthwise_W\n is multiplied by \nx\n to produce depthwise conolution, which is multiplied by\nthe \npointwise_W\n to produce a output \nTensor\n\nIf a \nbatch_norm\n is provided (such as\n\nbatch_norm\n), it is then applied. Otherwise, if \nbatch_norm\n is\nNone and a \nb_init\n and \nuse_bias\n is provided then a \nbiases\n variable would be\ncreated and added the hidden units. Finally, if \nactivation\n is not \nNone\n,\nit is applied to the hidden units as well.\nNote: that if \nx\n have a rank 4\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D \nTensor\n of with rank 4 and value for the last dimension,\ni.e. \n[batch_size, in_height, in_width, depth]\n,\n\n\nis_training\n: Bool, training or testing\n\n\nn_output\n: Integer or long, the number of output units in the layer.\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\n\n\nfilter_size\n: a list or tuple of 2 positive integers specifying the spatial\n\n\ndimensions of of the filters.\n\n\ndepth_multiplier\n:  A positive int32. the number of depthwise convolution output channels for\neach input channel. The total number of depthwise convolution output\nchannels will be equal to `num_filters_in * depth_multiplier\n\n\npadding\n: one of \n\"VALID\"\n or \n\"SAME\"\n.\n\n\nactivation\n: activation function, set to None to skip it and maintain\na linear activation.\n\n\nbatch_norm\n: normalization function to use. If\n\nbatch_norm\n is \nTrue\n then google original implementation is used and\nif another function is provided then it is applied.\ndefault set to None for no normalizer function\n\n\nbatch_norm_args\n: normalization function parameters.\n\n\nw_init\n: An initializer for the weights.\n\n\nw_regularizer\n: Optional regularizer for the weights.\n\n\nuntie_biases\n: spatial dimensions wise baises\n\n\nb_init\n: An initializer for the biases. If None skip biases.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\ntrainable\n: If \nTrue\n also add variables to the graph collection\n\nGraphKeys.TRAINABLE_VARIABLES\n (see tf.Variable).\n\n\nname\n: Optional name or scope for variable_scope/name_scope.\n\n\nuse_bias\n: Whether to add bias or not\n\n\n\n\nReturns\n\n\n\nThe 4-D \nTensor\n variable representing the result of the series of operations.\ne.g.: 4-D \nTensor\n [batch, new_height, new_width, n_output].\n\n\n\n\nAdds a 2D sdepthwise convolutional layer\n\n\ntefla.core.layers.depthwise_conv2d\n  (x,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  stride=  (1,  1),  depth_multiplier=8,  padding='SAME',  w_init=\n,  b_init=0.0,  w_regularizer=\n,  untie_biases=False,  name='depthwise_conv2d',  batch_norm=None,  batch_norm_args=None,  activation=None,  use_bias=True,  outputs_collections=None)\n\nGiven an input tensor of shape [batch, in_height, in_width, in_channels] and a filter\ntensor of shape [filter_height, filter_width, in_channels, channel_multiplier] containing\nin_channels convolutional filters of depth 1, depthwise_conv2d applies a different filter\nto each input channel (expanding from 1 channel to channel_multiplier channels for each),\nthen concatenates the results together. The output has in_channels * channel_multiplier channels.\nIf a \nbatch_norm\n is provided (such as\n\nbatch_norm\n), it is then applied. Otherwise, if \nbatch_norm\n is\nNone and a \nb_init\n and \nuse_bias\n is provided then a \nbiases\n variable would be\ncreated and added the hidden units. Finally, if \nactivation\n is not \nNone\n,\nit is applied to the hidden units as well.\nNote: that if \nx\n have a rank 4\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D \nTensor\n of with rank 4 and value for the last dimension,\ni.e. \n[batch_size, in_height, in_width, depth]\n,\n\n\nis_training\n: Bool, training or testing\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\n\n\nfilter_size\n: a list or tuple of 2 positive integers specifying the spatial\ndimensions of of the filters.\n\n\ndepth_multiplier\n:  A positive int32. the number of depthwise convolution output channels for\neach input channel. The total number of depthwise convolution output\nchannels will be equal to `num_filters_in * depth_multiplier\n\n\npadding\n: one of \n\"VALID\"\n or \n\"SAME\"\n.\n\n\nactivation\n: activation function, set to None to skip it and maintain\na linear activation.\n\n\nbatch_norm\n: normalization function to use. If\n\nbatch_norm\n is \nTrue\n then google original implementation is used and\nif another function is provided then it is applied.\ndefault set to None for no normalizer function\n\n\nbatch_norm_args\n: normalization function parameters.\n\n\nw_init\n: An initializer for the weights.\n\n\nw_regularizer\n: Optional regularizer for the weights.\n\n\nuntie_biases\n: spatial dimensions wise baises\n\n\nb_init\n: An initializer for the biases. If None skip biases.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\ntrainable\n: If \nTrue\n also add variables to the graph collection\n\nGraphKeys.TRAINABLE_VARIABLES\n (see tf.Variable).\n\n\nname\n: Optional name or scope for variable_scope/name_scope.\n\n\nuse_bias\n: Whether to add bias or not\n\n\n\n\nReturns\n\n\n\nThe tensor variable representing the result of the series of operations.\ne.g.: 4-D \nTensor\n [batch, new_height, new_width, n_output].\n\n\n\n\nAdds a 2D upsampling or deconvolutional layer\n\n\ntefla.core.layers.upsample2d\n  (input_,  output_shape,  is_training,  reuse,  trainable=True,  filter_size=  (5,  5),  stride=  (2,  2),  w_init=\n,  b_init=0.0,  w_regularizer=\n,  batch_norm=None,  activation=None,  name='deconv2d',  use_bias=True,  with_w=False,  outputs_collections=None,  **unused)\n\nhis operation is sometimes called \"deconvolution\" after Deconvolutional Networks,\nbut is actually the transpose (gradient) of conv2d rather than an actual deconvolution.\nIf a \nbatch_norm\n is provided (such as\n\nbatch_norm\n), it is then applied. Otherwise, if \nbatch_norm\n is\nNone and a \nb_init\n and \nuse_bias\n is provided then a \nbiases\n variable would be\ncreated and added the hidden units. Finally, if \nactivation\n is not \nNone\n,\nit is applied to the hidden units as well.\nNote: that if \nx\n have a rank 4\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D \nTensor\n of with at least rank 2 and value for the last dimension,\ni.e. \n[batch_size, in_height, in_width, depth]\n,\n\n\nis_training\n: Bool, training or testing\n\n\noutput_shape\n: 4D tensor, the output shape\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\n\n\nfilter_size\n: a list or tuple of 2 positive integers specifying the spatial\ndimensions of of the filters.\n\n\nstride\n: a tuple or list of 2 positive integers specifying the stride at which to\ncompute output.\n\n\npadding\n: one of \n\"VALID\"\n or \n\"SAME\"\n.\n\n\nactivation\n: activation function, set to None to skip it and maintain\na linear activation.\n\n\nbatch_norm\n: normalization function to use. If\n\nbatch_norm\n is \nTrue\n then google original implementation is used and\nif another function is provided then it is applied.\ndefault set to None for no normalizer function\n\n\nbatch_norm_args\n: normalization function parameters.\n\n\nw_init\n: An initializer for the weights.\n\n\nw_regularizer\n: Optional regularizer for the weights.\n\n\nb_init\n: An initializer for the biases. If None skip biases.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\ntrainable\n: If \nTrue\n also add variables to the graph collection\n\nGraphKeys.TRAINABLE_VARIABLES\n (see tf.Variable).\n\n\nname\n: Optional name or scope for variable_scope/name_scope.\n\n\nuse_bias\n: Whether to add bias or not\n\n\n\n\nReturns\n\n\n\nThe tensor variable representing the result of the series of operations.\ne.g.: 4-D \nTensor\n [batch, new_height, new_width, n_output].\n\n\n\n\nAdds a 2D highway convolutional layer\n\n\ntefla.core.layers.highway_conv2d\n  (x,  n_output,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  stride=  (1,  1),  padding='SAME',  w_init=\n,  b_init=0.0,  w_regularizer=\n,  name='highway_conv2d',  activation=None,  use_bias=True,  outputs_collections=None)\n\nhttps://arxiv.org/abs/1505.00387\nIf a \nbatch_norm\n is provided (such as\n\nbatch_norm\n), it is then applied. Otherwise, if \nbatch_norm\n is\nNone and a \nb_init\n and \nuse_bias\n is provided then a \nbiases\n variable would be\ncreated and added the hidden units. Finally, if \nactivation\n is not \nNone\n,\nit is applied to the hidden units as well.\nNote: that if \nx\n have a rank 4\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D \nTensor\n of with at least rank 2 and value for the last dimension,\ni.e. \n[batch_size, in_height, in_width, depth]\n,\n\n\nis_training\n: Bool, training or testing\n\n\nn_output\n: Integer or long, the number of output units in the layer.\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\n\n\nfilter_size\n: a list or tuple of 2 positive integers specifying the spatial\ndimensions of of the filters.\n\n\nstride\n: a tuple or list of 2 positive integers specifying the stride at which to\ncompute output.\n\n\npadding\n: one of \n\"VALID\"\n or \n\"SAME\"\n.\n\n\nactivation\n: activation function, set to None to skip it and maintain\na linear activation.\n\n\nbatch_norm\n: normalization function to use. If\n\nbatch_norm\n is \nTrue\n then google original implementation is used and\nif another function is provided then it is applied.\ndefault set to None for no normalizer function\n\n\nbatch_norm_args\n: normalization function parameters.\n\n\nw_init\n: An initializer for the weights.\n\n\nw_regularizer\n: Optional regularizer for the weights.\n\n\nuntie_biases\n: spatial dimensions wise baises\n\n\nb_init\n: An initializer for the biases. If None skip biases.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\ntrainable\n: If \nTrue\n also add variables to the graph collection\n\nGraphKeys.TRAINABLE_VARIABLES\n (see tf.Variable).\n\n\nname\n: Optional name or scope for variable_scope/name_scope.\n\n\nuse_bias\n: Whether to add bias or not\n\n\n\n\nReturns\n\n\n\nThe \nTensor\n variable representing the result of the series of operations.\ne.g.: 4-D \nTensor\n [batch, new_height, new_width, n_output].\n\n\n\n\nAdds a fully connected highway layer\n\n\ntefla.core.layers.highway_fc2d\n  (x,  n_output,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  w_init=\n,  b_init=0.0,  w_regularizer=\n,  name='highway_fc2d',  activation=None,  use_bias=True,  outputs_collections=None)\n\nhttps://arxiv.org/abs/1505.00387\nIf a \nbatch_norm\n is provided (such as\n\nbatch_norm\n), it is then applied. Otherwise, if \nbatch_norm\n is\nNone and a \nb_init\n and \nuse_bias\n is provided then a \nbiases\n variable would be\ncreated and added the hidden units. Finally, if \nactivation\n is not \nNone\n,\nit is applied to the hidden units as well.\nNote: that if \nx\n have a rank greater than 2, then \nx\n is flattened\nprior to the initial matrix multiply by \nweights\n.\n\n\nArgs\n\n\n\n\n\nx\n: A 2-D/4-D \nTensor\n of with at least rank 2 and value for the last dimension,\ni.e. \n[batch_size, depth]\n, \n[None, None, None, channels]\n.\n\n\nis_training\n: Bool, training or testing\n\n\nn_output\n: Integer or long, the number of output units in the layer.\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\n\n\nactivation\n: activation function, set to None to skip it and maintain\na linear activation.\n\n\nbatch_norm\n: normalization function to use. If\n\nbatch_norm\n is \nTrue\n then google original implementation is used and\nif another function is provided then it is applied.\ndefault set to None for no normalizer function\n\n\nbatch_norm_args\n: normalization function parameters.\n\n\nw_init\n: An initializer for the weights.\n\n\nw_regularizer\n: Optional regularizer for the weights.\n\n\nb_init\n: An initializer for the biases. If None skip biases.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\ntrainable\n: If \nTrue\n also add variables to the graph collection\n\nGraphKeys.TRAINABLE_VARIABLES\n (see tf.Variable).\n\n\nname\n: Optional name or scope for variable_scope/name_scope.\n\n\nuse_bias\n: Whether to add bias or not\n\n\n\n\nReturns\n\n\n\nThe 2-D \nTensor\n variable representing the result of the series of operations.\ne.g.: 2-D \nTensor\n [batch_size, n_output]\n\n\n\n\nMax pooling layer\n\n\ntefla.core.layers.max_pool\n  (x,  filter_size=  (3,  3),  stride=  (2,  2),  padding='SAME',  name='pool',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D 'Tensor\nof shape\n[batch_size, height, width, channels]`\n\n\nfilter_size\n: A list of length 2: [kernel_height, kernel_width] of the\npooling kernel over which the op is computed. Can be an int if both\nvalues are the same.\n\n\nstride\n: A list of length 2: [stride_height, stride_width].\n\n\npadding\n: The padding method, either 'VALID' or 'SAME'.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\nname\n: Optional scope/name for name_scope.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the pooling operation.\ne.g.: 4-D \nTensor\n [batch, new_height, new_width, channels].\n\n\n\n\nFractional pooling layer\n\n\ntefla.core.layers.fractional_pool\n  (x,  pooling_ratio=[1.0,  1.44,  1.73,  1.0],  pseudo_random=None,  determinastic=None,  overlapping=None,  name='fractional_pool',  seed=None,  seed2=None,  type='avg',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D \nTensor\n of shape \n[batch_size, height, width, channels]\n\n\npooling_ratio\n: A list of floats that has length \n= 4. Pooling ratio for each\ndimension of value, currently only supports row and col dimension and should\nbe \n= 1.0. For example, a valid pooling ratio looks like [1.0, 1.44, 1.73, 1.0].\nThe first and last elements must be 1.0 because we don't allow pooling on batch and\nchannels dimensions. 1.44 and 1.73 are pooling ratio on height and width\ndimensions respectively.\n\n\npseudo_random\n: An optional bool. Defaults to False. When set to True, generates\nthe pooling sequence in a pseudorandom fashion, otherwise, in a random fashion.\nCheck paper Benjamin Graham, Fractional Max-Pooling for difference between\npseudorandom and random.\n\n\noverlapping\n: An optional bool. Defaults to False. When set to True, it means when pooling,\nthe values at the boundary of adjacent pooling cells are used by both cells.\nFor example: index 0 1 2 3 4\nvalue 20 5 16 3 7; If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used\ntwice. The result would be [41/3, 26/3] for fractional avg pooling.\n\n\ndeterministic\n: An optional bool. Defaults to False. When set to True, a fixed pooling\nregion will be used when iterating over a FractionalAvgPool node in the computation\ngraph. Mainly used in unit test to make FractionalAvgPool deterministic.\n\n\nseed\n: An optional int. Defaults to 0. If either seed or seed2 are set to be non-zero,\nthe random number generator is seeded by the given seed. Otherwise,\nit is seeded by a random seed.\n\n\nseed2\n: An optional int. Defaults to 0. An second seed to avoid seed collision.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\ntype\n: avg or max pool\n\n\nname\n: Optional scope/name for name_scope.\n\n\n\n\nReturns\n\n\n\nA 4-D \nTensor\n representing the results of the pooling operation.\ne.g.: 4-D \nTensor\n [batch, new_height, new_width, channels].\n\n\n\n\nRMS pooling layer\n\n\ntefla.core.layers.rms_pool_2d\n  (x,  filter_size=  (3,  3),  stride=  (2,  2),  padding='SAME',  name='pool',  epsilon=1e-12,  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D \nTensor\n of shape \n[batch_size, height, width, channels]\n\n\nfilter_size\n: A list of length 2: [kernel_height, kernel_width] of the\npooling kernel over which the op is computed. Can be an int if both\nvalues are the same.\n\n\nstride\n: A list of length 2: [stride_height, stride_width].\n\n\npadding\n: The padding method, either 'VALID' or 'SAME'.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\nname\n: Optional scope/name for name_scope.\n\n\nepsilon\n: prevents divide by zero\n\n\n\n\nReturns\n\n\n\nA 4-D \nTensor\n representing the results of the pooling operation.\ne.g.: 4-D \nTensor\n [batch, new_height, new_width, channels].\n\n\n\n\nAvg pooling layer\n\n\ntefla.core.layers.avg_pool_2d\n  (x,  filter_size=  (3,  3),  stride=  (2,  2),  padding='SAME',  name=None,  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D \nTensor\n of shape \n[batch_size, height, width, channels]\n\n\nfilter_size\n: A list of length 2: [kernel_height, kernel_width] of the\npooling kernel over which the op is computed. Can be an int if both\nvalues are the same.\n\n\nstride\n: A list of length 2: [stride_height, stride_width].\n\n\npadding\n: The padding method, either 'VALID' or 'SAME'.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\nname\n: Optional scope/name for name_scope.\n\n\n\n\nReturns\n\n\n\nA 4-D \nTensor\n representing the results of the pooling operation.\ne.g.: 4-D \nTensor\n [batch, new_height, new_width, channels].\n\n\n\n\nGloabl pooling layer\n\n\ntefla.core.layers.global_avg_pool\n  (x,  name='global_avg_pool',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: A 4-D \nTensor\n of shape \n[batch_size, height, width, channels]\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\nname\n: Optional scope/name for name_scope.\n\n\n\n\nReturns\n\n\n\nA 4-D \nTensor\n representing the results of the pooling operation.\ne.g.: 4-D \nTensor\n [batch, 1, 1, channels].\n\n\n\n\nFeature max pooling layer\n\n\ntefla.core.layers.feature_max_pool_1d\n  (x,  stride=2,  name='pool',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: A 2-D tensor of shape \n[batch_size, channels]\n\n\nstride\n: A int.\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\nname\n: Optional scope/name for name_scope.\n\n\n\n\nReturns\n\n\n\nA 2-D \nTensor\n representing the results of the pooling operation.\ne.g.: 2-D \nTensor\n [batch_size, new_channels]\n\n\n\n\nAdds a Batch Normalization layer from http://arxiv.org/abs/1502.03167\n\n\ntefla.core.layers.batch_norm_tf\n  (x,  name='bn',  scale=False,  updates_collections=None,  **kwargs)\n\n\"Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift\", Sergey Ioffe, Christian Szegedy\nCan be used as a normalizer function for conv2d and fully_connected.\nNote: When is_training is True the moving_mean and moving_variance need to be\nupdated, by default the update_ops are placed in \ntf.GraphKeys.UPDATE_OPS\n so\nthey need to be added as a dependency to the \ntrain_op\n, example:\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n\nif update_ops:\n\n\nupdates = tf.group(*update_ops)\n\n\ntotal_loss = control_flow_ops.with_dependencies([updates], total_loss)\n\nOne can set updates_collections=None to force the updates in place, but that\ncan have speed penalty, specially in distributed settings.\n\nArgs\n\n\n\n\nx\n: a \nTensor\n with 2 or more dimensions, where the first dimension has\n\nbatch_size\n. The normalization is over all but the last dimension if\n\ndata_format\n is \nNHWC\n and the second dimension if \ndata_format\n is\n\nNCHW\n.\n\n\ndecay\n: decay for the moving average. Reasonable values for \ndecay\n are close\nto 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\nLower \ndecay\n value (recommend trying \ndecay\n=0.9) if model experiences\nreasonably good training performance but poor validation and/or test\nperformance. Try zero_debias_moving_mean=True for improved stability.\n\n\ncenter\n: If True, subtract \nbeta\n. If False, \nbeta\n is ignored.\n\n\nscale\n: If True, multiply by \ngamma\n. If False, \ngamma\n is\nnot used. When the next layer is linear (also e.g. \nnn.relu\n), this can be\ndisabled since the scaling can be done by the next layer.\n\n\nepsilon\n: small float added to variance to avoid dividing by zero.\n\n\nactivation_fn\n: activation function, default set to None to skip it and\nmaintain a linear activation.\n\n\nparam_initializers\n: optional initializers for beta, gamma, moving mean and\nmoving variance.\n\n\nupdates_collections\n: collections to collect the update ops for computation.\nThe updates_ops need to be executed with the train_op.\nIf None, a control dependency would be added to make sure the updates are\ncomputed in place.\n\n\nis_training\n: whether or not the layer is in training mode. In training mode\nit would accumulate the statistics of the moments into \nmoving_mean\n and\n\nmoving_variance\n using an exponential moving average with the given\n\ndecay\n. When it is not in training mode then it would use the values of\nthe \nmoving_mean\n and the \nmoving_variance\n.\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\noutputs_collections: collections to add the outputs.\n\n\ntrainable\n: If \nTrue\n also add variables to the graph collection\n\nGraphKeys.TRAINABLE_VARIABLES\n (see \ntf.Variable\n).\n\n\nbatch_weights\n: An optional tensor of shape \n[batch_size]\n,\ncontaining a frequency weight for each batch item. If present,\nthen the batch normalization uses weighted mean and\nvariance. (This can be used to correct for bias in training\nexample selection.)\n\n\nfused\n:  Use nn.fused_batch_norm if True, nn.batch_normalization otherwise.\n\n\nname\n: Optional scope/name for \nvariable_scope\n.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the output of the operation.\n\n\n\n\nAdds a Batch Normalization layer from http://arxiv.org/abs/1502.03167\n\n\ntefla.core.layers.batch_norm_lasagne\n  (x,  is_training,  reuse,  trainable=True,  decay=0.9,  epsilon=0.0001,  name='bn',  updates_collections='update_ops',  outputs_collections=None)\n\nInstead of storing and updating moving variance, this layer store and\nupdate moving inverse standard deviation\n\"Batch Normalization: Accelerating Deep Network Training by Reducin Internal Covariate Shift\"\nSergey Ioffe, Christian Szegedy\nCan be used as a normalizer function for conv2d and fully_connected.\nNote: When is_training is True the moving_mean and moving_variance need to be\nupdated, by default the update_ops are placed in \ntf.GraphKeys.UPDATE_OPS\n so\nthey need to be added as a dependency to the \ntrain_op\n, example:\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n\nif update_ops:\n\n\nupdates = tf.group(*update_ops)\n\n\ntotal_loss = control_flow_ops.with_dependencies([updates], total_loss)\n\nOne can set updates_collections=None to force the updates in place, but that\ncan have speed penalty, specially in distributed settings.\n\n\nArgs\n\n\n\n\n\nx\n: a tensor with 2 or more dimensions, where the first dimension has\n\nbatch_size\n. The normalization is over all but the last dimension if\n\ndata_format\n is \nNHWC\n and the second dimension if \ndata_format\n is\n\nNCHW\n.\n\n\ndecay\n: decay for the moving average. Reasonable values for \ndecay\n are close\nto 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\nLower \ndecay\n value (recommend trying \ndecay\n=0.9) if model experiences\nreasonably good training performance but poor validation and/or test\nperformance. Try zero_debias_moving_mean=True for improved stability.\n\n\nepsilon\n: small float added to variance to avoid dividing by zero.\n\n\nupdates_collections\n: collections to collect the update ops for computation.\nThe updates_ops need to be executed with the train_op.\nIf None, a control dependency would be added to make sure the updates are\ncomputed in place.\n\n\nis_training\n: whether or not the layer is in training mode. In training mode\nit would accumulate the statistics of the moments into \nmoving_mean\n and\n\nmoving_variance\n using an exponential moving average with the given\n\ndecay\n. When it is not in training mode then it would use the values of\nthe \nmoving_mean\n and the \nmoving_variance\n.\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\n\n\noutputs_collections\n: collections to add the outputs.\n\n\ntrainable\n: If \nTrue\n also add variables to the graph collection\n\nGraphKeys.TRAINABLE_VARIABLES\n (see \ntf.Variable\n).\n\n\nname\n: Optional scope/name for \nvariable_scope\n.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the output of the operation.\n\n\n\n\nPrametric rectifier linear layer\n\n\ntefla.core.layers.prelu\n  (x,  reuse,  alpha_init=0.2,  trainable=True,  name='prelu',  outputs_collections=None)\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\nreuse\n: whether or not the layer and its variables should be reused. To be\nable to reuse the layer scope must be given.\n\n\nalpha_init\n: initalization value for alpha\n\n\ntrainable\n: a bool, training or fixed value\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the prelu activation operation.\n\n\n\n\nRectifier linear layer\n\n\ntefla.core.layers.relu\n  (x,  name='relu',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the relu activation operation.\n\n\n\n\nRectifier linear relu6 layer\n\n\ntefla.core.layers.relu6\n  (x,  name='relu6',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the relu6 activation operation.\n\n\n\n\nSoftpluas layer\n\n\ntefla.core.layers.softplus\n  (x,  name='softplus',  outputs_collections=None,  **unused)\n\nComputes softplus: log(exp(features) + 1).\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the activation operation.\n\n\n\n\nComputes Concatenated ReLU\n\n\ntefla.core.layers.crelu\n  (x,  name='crelu',  outputs_collections=None,  **unused)\n\nConcatenates a ReLU which selects only the positive part of the activation with\na ReLU which selects only the negative part of the activation. Note that\nat as a result this non-linearity doubles the depth of the activations.\nSource: https://arxiv.org/abs/1603.05201\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the activation operation.\n\n\n\n\nComputes exponential linear: exp(features) - 1 if \n 0, features otherwise\n\n\ntefla.core.layers.elu\n  (x,  name='elu',  outputs_collections=None,  **unused)\n\nSee \"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\"\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the activation operation.\n\n\n\n\nComputes reaky relu\n\n\ntefla.core.layers.leaky_relu\n  (x,  alpha=0.01,  name='leaky_relu',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\naplha\n: the conatant fro scalling the activation\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the activation operation.\n\n\n\n\nComputes reaky relu lasagne style\n\n\ntefla.core.layers.lrelu\n  (x,  leak=0.2,  name='lrelu',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\nleak\n: the conatant fro scalling the activation\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the activation operation.\n\n\n\n\nComputes maxout activation\n\n\ntefla.core.layers.maxout\n  (x,  k=2,  name='maxout',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\nk\n: output channel splitting factor\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the activation operation.\n\n\n\n\nComputes maxout activation\n\n\ntefla.core.layers.offset_maxout\n  (x,  k=2,  name='maxout',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\nk\n: output channel splitting factor\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the activation operation.\n\n\n\n\nComputes softmax activation\n\n\ntefla.core.layers.softmax\n  (x,  name='softmax',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n, int16\n, or\nint8`.\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the activation operation.\n\n\n\n\nDropout layer\n\n\ntefla.core.layers.dropout\n  (x,  is_training,  drop_p=0.5,  name='dropout',  outputs_collections=None,  **unused)\n\n\nArgs\n\n\n\n\nx\n: a \nTensor\n.\n\n\nis_training\n: a bool, training or validation\n\n\ndrop_p\n: probability of droping unit\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the dropout operation.\n\n\n\n\nRepeat op\n\n\ntefla.core.layers.repeat\n  (x,  repetitions,  layer,  name='repeat',  outputs_collections=None,  \nargs,  \n*kwargs)\n\n\nArgs\n\n\n\n\n\nx\n: a \nTensor\n.\n\n\nrepetitions\n: a int, number of times to apply the same operation\n\n\nlayer\n: the layer function with arguments to repeat\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the repetition operation.\n\n\n\n\nMerge op\n\n\ntefla.core.layers.merge\n  (tensors_list,  mode,  axis=1,  name='merge',  outputs_collections=None,  **kwargs)\n\n\nArgs\n\n\n\n\n\ntensor_list\n: A list \nTensors\n to merge\n\n\nmode\n: str, available modes are\n['concat', 'elemwise_sum', 'elemwise_mul', 'sum', 'mean', 'prod', 'max', 'min', 'and', 'or']\n\n\nname\n: a optional scope/name of the layer\n\n\noutputs_collections\n: The collections to which the outputs are added.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n representing the results of the repetition operation.", 
            "title": "Layers"
        }, 
        {
            "location": "/core/layers/#define-input-layer", 
            "text": "tefla.core.layers.input   (shape,  name='inputs',  outputs_collections=None,  **unused)", 
            "title": "Define input layer"
        }, 
        {
            "location": "/core/layers/#adds-a-fully-connected-layer", 
            "text": "tefla.core.layers.fully_connected   (x,  n_output,  is_training,  reuse,  trainable=True,  w_init= ,  b_init=0.0,  w_regularizer= ,  name='fc',  batch_norm=None,  batch_norm_args=None,  activation=None,  outputs_collections=None,  use_bias=True)  fully_connected  creates a variable called  weights , representing a fully\nconnected weight matrix, which is multiplied by the  x  to produce a Tensor  of hidden units. If a  batch_norm  is provided (such as batch_norm ), it is then applied. Otherwise, if  batch_norm  is\nNone and a  b_init  and  use_bias  is provided then a  biases  variable would be\ncreated and added the hidden units. Finally, if  activation  is not  None ,\nit is applied to the hidden units as well.\nNote: that if  x  have a rank greater than 2, then  x  is flattened\nprior to the initial matrix multiply by  weights .", 
            "title": "Adds a fully connected layer"
        }, 
        {
            "location": "/core/layers/#adds-a-2d-convolutional-layer", 
            "text": "tefla.core.layers.conv2d   (x,  n_output_channels,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  stride=  (1,  1),  padding='SAME',  w_init= ,  b_init=0.0,  w_regularizer= ,  untie_biases=False,  name='conv2d',  batch_norm=None,  batch_norm_args=None,  activation=None,  use_bias=True,  outputs_collections=None)  convolutional layer  creates a variable called  weights , representing a conv\nweight matrix, which is multiplied by the  x  to produce a Tensor  of hidden units. If a  batch_norm  is provided (such as batch_norm ), it is then applied. Otherwise, if  batch_norm  is\nNone and a  b_init  and  use_bias  is provided then a  biases  variable would be\ncreated and added the hidden units. Finally, if  activation  is not  None ,\nit is applied to the hidden units as well.\nNote: that if  x  have a rank 4", 
            "title": "Adds a 2D convolutional layer"
        }, 
        {
            "location": "/core/layers/#adds-a-2d-dilated-convolutional-layer-also-known-as-convolution-with-holes-or-atrous-convolution", 
            "text": "tefla.core.layers.dilated_conv2d   (x,  n_output_channels,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  dilation=1,  padding='SAME',  w_init= ,  b_init=0.0,  w_regularizer= ,  untie_biases=False,  name='dilated_conv2d',  batch_norm=None,  batch_norm_args=None,  activation=None,  use_bias=True,  outputs_collections=None) \nIf the rate parameter is equal to one, it performs regular 2-D convolution. If the rate parameter\nis greater than one, it performs convolution with holes, sampling the input\nvalues every rate pixels in the height and width dimensions. convolutional layer  creates a variable called  weights , representing a conv\nweight matrix, which is multiplied by the  x  to produce a Tensor  of hidden units. If a  batch_norm  is provided (such as batch_norm ), it is then applied. Otherwise, if  batch_norm  is\nNone and a  b_init  and  use_bias  is provided then a  biases  variable would be\ncreated and added the hidden units. Finally, if  activation  is not  None ,\nit is applied to the hidden units as well.\nNote: that if  x  have a rank 4", 
            "title": "Adds a 2D dilated convolutional layer, also known as convolution with holes or atrous convolution"
        }, 
        {
            "location": "/core/layers/#adds-a-2d-seperable-convolutional-layer", 
            "text": "tefla.core.layers.separable_conv2d   (x,  n_output_channels,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  stride=  (1,  1),  depth_multiplier=8,  padding='SAME',  w_init= ,  b_init=0.0,  w_regularizer= ,  untie_biases=False,  name='separable_conv2d',  batch_norm=None,  batch_norm_args=None,  activation=None,  use_bias=True,  outputs_collections=None) \nPerforms a depthwise convolution that acts separately on channels followed by\na pointwise convolution that mixes channels. Note that this is separability between\ndimensions [1, 2] and 3, not spatial separability between dimensions 1 and 2. convolutional layer  creates two variable called  depthwise_W  and  pointwise_W , depthwise_W  is multiplied by  x  to produce depthwise conolution, which is multiplied by\nthe  pointwise_W  to produce a output  Tensor \nIf a  batch_norm  is provided (such as batch_norm ), it is then applied. Otherwise, if  batch_norm  is\nNone and a  b_init  and  use_bias  is provided then a  biases  variable would be\ncreated and added the hidden units. Finally, if  activation  is not  None ,\nit is applied to the hidden units as well.\nNote: that if  x  have a rank 4", 
            "title": "Adds a 2D seperable convolutional layer"
        }, 
        {
            "location": "/core/layers/#adds-a-2d-sdepthwise-convolutional-layer", 
            "text": "tefla.core.layers.depthwise_conv2d   (x,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  stride=  (1,  1),  depth_multiplier=8,  padding='SAME',  w_init= ,  b_init=0.0,  w_regularizer= ,  untie_biases=False,  name='depthwise_conv2d',  batch_norm=None,  batch_norm_args=None,  activation=None,  use_bias=True,  outputs_collections=None) \nGiven an input tensor of shape [batch, in_height, in_width, in_channels] and a filter\ntensor of shape [filter_height, filter_width, in_channels, channel_multiplier] containing\nin_channels convolutional filters of depth 1, depthwise_conv2d applies a different filter\nto each input channel (expanding from 1 channel to channel_multiplier channels for each),\nthen concatenates the results together. The output has in_channels * channel_multiplier channels.\nIf a  batch_norm  is provided (such as batch_norm ), it is then applied. Otherwise, if  batch_norm  is\nNone and a  b_init  and  use_bias  is provided then a  biases  variable would be\ncreated and added the hidden units. Finally, if  activation  is not  None ,\nit is applied to the hidden units as well.\nNote: that if  x  have a rank 4", 
            "title": "Adds a 2D sdepthwise convolutional layer"
        }, 
        {
            "location": "/core/layers/#adds-a-2d-upsampling-or-deconvolutional-layer", 
            "text": "tefla.core.layers.upsample2d   (input_,  output_shape,  is_training,  reuse,  trainable=True,  filter_size=  (5,  5),  stride=  (2,  2),  w_init= ,  b_init=0.0,  w_regularizer= ,  batch_norm=None,  activation=None,  name='deconv2d',  use_bias=True,  with_w=False,  outputs_collections=None,  **unused) \nhis operation is sometimes called \"deconvolution\" after Deconvolutional Networks,\nbut is actually the transpose (gradient) of conv2d rather than an actual deconvolution.\nIf a  batch_norm  is provided (such as batch_norm ), it is then applied. Otherwise, if  batch_norm  is\nNone and a  b_init  and  use_bias  is provided then a  biases  variable would be\ncreated and added the hidden units. Finally, if  activation  is not  None ,\nit is applied to the hidden units as well.\nNote: that if  x  have a rank 4", 
            "title": "Adds a 2D upsampling or deconvolutional layer"
        }, 
        {
            "location": "/core/layers/#adds-a-2d-highway-convolutional-layer", 
            "text": "tefla.core.layers.highway_conv2d   (x,  n_output,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  stride=  (1,  1),  padding='SAME',  w_init= ,  b_init=0.0,  w_regularizer= ,  name='highway_conv2d',  activation=None,  use_bias=True,  outputs_collections=None) \nhttps://arxiv.org/abs/1505.00387\nIf a  batch_norm  is provided (such as batch_norm ), it is then applied. Otherwise, if  batch_norm  is\nNone and a  b_init  and  use_bias  is provided then a  biases  variable would be\ncreated and added the hidden units. Finally, if  activation  is not  None ,\nit is applied to the hidden units as well.\nNote: that if  x  have a rank 4", 
            "title": "Adds a 2D highway convolutional layer"
        }, 
        {
            "location": "/core/layers/#adds-a-fully-connected-highway-layer", 
            "text": "tefla.core.layers.highway_fc2d   (x,  n_output,  is_training,  reuse,  trainable=True,  filter_size=  (3,  3),  w_init= ,  b_init=0.0,  w_regularizer= ,  name='highway_fc2d',  activation=None,  use_bias=True,  outputs_collections=None) \nhttps://arxiv.org/abs/1505.00387\nIf a  batch_norm  is provided (such as batch_norm ), it is then applied. Otherwise, if  batch_norm  is\nNone and a  b_init  and  use_bias  is provided then a  biases  variable would be\ncreated and added the hidden units. Finally, if  activation  is not  None ,\nit is applied to the hidden units as well.\nNote: that if  x  have a rank greater than 2, then  x  is flattened\nprior to the initial matrix multiply by  weights .", 
            "title": "Adds a fully connected highway layer"
        }, 
        {
            "location": "/core/layers/#max-pooling-layer", 
            "text": "tefla.core.layers.max_pool   (x,  filter_size=  (3,  3),  stride=  (2,  2),  padding='SAME',  name='pool',  outputs_collections=None,  **unused)", 
            "title": "Max pooling layer"
        }, 
        {
            "location": "/core/layers/#fractional-pooling-layer", 
            "text": "tefla.core.layers.fractional_pool   (x,  pooling_ratio=[1.0,  1.44,  1.73,  1.0],  pseudo_random=None,  determinastic=None,  overlapping=None,  name='fractional_pool',  seed=None,  seed2=None,  type='avg',  outputs_collections=None,  **unused)", 
            "title": "Fractional pooling layer"
        }, 
        {
            "location": "/core/layers/#rms-pooling-layer", 
            "text": "tefla.core.layers.rms_pool_2d   (x,  filter_size=  (3,  3),  stride=  (2,  2),  padding='SAME',  name='pool',  epsilon=1e-12,  outputs_collections=None,  **unused)", 
            "title": "RMS pooling layer"
        }, 
        {
            "location": "/core/layers/#avg-pooling-layer", 
            "text": "tefla.core.layers.avg_pool_2d   (x,  filter_size=  (3,  3),  stride=  (2,  2),  padding='SAME',  name=None,  outputs_collections=None,  **unused)", 
            "title": "Avg pooling layer"
        }, 
        {
            "location": "/core/layers/#gloabl-pooling-layer", 
            "text": "tefla.core.layers.global_avg_pool   (x,  name='global_avg_pool',  outputs_collections=None,  **unused)", 
            "title": "Gloabl pooling layer"
        }, 
        {
            "location": "/core/layers/#feature-max-pooling-layer", 
            "text": "tefla.core.layers.feature_max_pool_1d   (x,  stride=2,  name='pool',  outputs_collections=None,  **unused)", 
            "title": "Feature max pooling layer"
        }, 
        {
            "location": "/core/layers/#adds-a-batch-normalization-layer-from-httparxivorgabs150203167", 
            "text": "tefla.core.layers.batch_norm_tf   (x,  name='bn',  scale=False,  updates_collections=None,  **kwargs) \n\"Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift\", Sergey Ioffe, Christian Szegedy\nCan be used as a normalizer function for conv2d and fully_connected.\nNote: When is_training is True the moving_mean and moving_variance need to be\nupdated, by default the update_ops are placed in  tf.GraphKeys.UPDATE_OPS  so\nthey need to be added as a dependency to the  train_op , example: update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  if update_ops:  updates = tf.group(*update_ops)  total_loss = control_flow_ops.with_dependencies([updates], total_loss) \nOne can set updates_collections=None to force the updates in place, but that\ncan have speed penalty, specially in distributed settings.", 
            "title": "Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167"
        }, 
        {
            "location": "/core/layers/#adds-a-batch-normalization-layer-from-httparxivorgabs150203167_1", 
            "text": "tefla.core.layers.batch_norm_lasagne   (x,  is_training,  reuse,  trainable=True,  decay=0.9,  epsilon=0.0001,  name='bn',  updates_collections='update_ops',  outputs_collections=None) \nInstead of storing and updating moving variance, this layer store and\nupdate moving inverse standard deviation\n\"Batch Normalization: Accelerating Deep Network Training by Reducin Internal Covariate Shift\"\nSergey Ioffe, Christian Szegedy\nCan be used as a normalizer function for conv2d and fully_connected.\nNote: When is_training is True the moving_mean and moving_variance need to be\nupdated, by default the update_ops are placed in  tf.GraphKeys.UPDATE_OPS  so\nthey need to be added as a dependency to the  train_op , example: update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  if update_ops:  updates = tf.group(*update_ops)  total_loss = control_flow_ops.with_dependencies([updates], total_loss) \nOne can set updates_collections=None to force the updates in place, but that\ncan have speed penalty, specially in distributed settings.", 
            "title": "Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167"
        }, 
        {
            "location": "/core/layers/#prametric-rectifier-linear-layer", 
            "text": "tefla.core.layers.prelu   (x,  reuse,  alpha_init=0.2,  trainable=True,  name='prelu',  outputs_collections=None)", 
            "title": "Prametric rectifier linear layer"
        }, 
        {
            "location": "/core/layers/#rectifier-linear-layer", 
            "text": "tefla.core.layers.relu   (x,  name='relu',  outputs_collections=None,  **unused)", 
            "title": "Rectifier linear layer"
        }, 
        {
            "location": "/core/layers/#rectifier-linear-relu6-layer", 
            "text": "tefla.core.layers.relu6   (x,  name='relu6',  outputs_collections=None,  **unused)", 
            "title": "Rectifier linear relu6 layer"
        }, 
        {
            "location": "/core/layers/#softpluas-layer", 
            "text": "tefla.core.layers.softplus   (x,  name='softplus',  outputs_collections=None,  **unused) \nComputes softplus: log(exp(features) + 1).", 
            "title": "Softpluas layer"
        }, 
        {
            "location": "/core/layers/#computes-concatenated-relu", 
            "text": "tefla.core.layers.crelu   (x,  name='crelu',  outputs_collections=None,  **unused) \nConcatenates a ReLU which selects only the positive part of the activation with\na ReLU which selects only the negative part of the activation. Note that\nat as a result this non-linearity doubles the depth of the activations.\nSource: https://arxiv.org/abs/1603.05201", 
            "title": "Computes Concatenated ReLU"
        }, 
        {
            "location": "/core/layers/#computes-exponential-linear-expfeatures-1-if-0-features-otherwise", 
            "text": "tefla.core.layers.elu   (x,  name='elu',  outputs_collections=None,  **unused) \nSee \"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\"", 
            "title": "Computes exponential linear: exp(features) - 1 if &lt; 0, features otherwise"
        }, 
        {
            "location": "/core/layers/#computes-reaky-relu", 
            "text": "tefla.core.layers.leaky_relu   (x,  alpha=0.01,  name='leaky_relu',  outputs_collections=None,  **unused)", 
            "title": "Computes reaky relu"
        }, 
        {
            "location": "/core/layers/#computes-reaky-relu-lasagne-style", 
            "text": "tefla.core.layers.lrelu   (x,  leak=0.2,  name='lrelu',  outputs_collections=None,  **unused)", 
            "title": "Computes reaky relu lasagne style"
        }, 
        {
            "location": "/core/layers/#computes-maxout-activation", 
            "text": "tefla.core.layers.maxout   (x,  k=2,  name='maxout',  outputs_collections=None,  **unused)", 
            "title": "Computes maxout activation"
        }, 
        {
            "location": "/core/layers/#computes-maxout-activation_1", 
            "text": "tefla.core.layers.offset_maxout   (x,  k=2,  name='maxout',  outputs_collections=None,  **unused)", 
            "title": "Computes maxout activation"
        }, 
        {
            "location": "/core/layers/#computes-softmax-activation", 
            "text": "tefla.core.layers.softmax   (x,  name='softmax',  outputs_collections=None,  **unused)", 
            "title": "Computes softmax activation"
        }, 
        {
            "location": "/core/layers/#dropout-layer", 
            "text": "tefla.core.layers.dropout   (x,  is_training,  drop_p=0.5,  name='dropout',  outputs_collections=None,  **unused)", 
            "title": "Dropout layer"
        }, 
        {
            "location": "/core/layers/#repeat-op", 
            "text": "tefla.core.layers.repeat   (x,  repetitions,  layer,  name='repeat',  outputs_collections=None,   args,   *kwargs)", 
            "title": "Repeat op"
        }, 
        {
            "location": "/core/layers/#merge-op", 
            "text": "tefla.core.layers.merge   (tensors_list,  mode,  axis=1,  name='merge',  outputs_collections=None,  **kwargs)", 
            "title": "Merge op"
        }, 
        {
            "location": "/core/metrics/", 
            "text": "Computes accuracy metric\n\n\ntefla.core.metrics.accuracy_op\n  (predictions,  targets,  num_classes=5)\n\n\nArgs\n\n\n\n\n\npredictions\n: 2D tensor/array, predictions of the network\n\n\ntargets\n: 2D tensor/array, ground truth labels of the network\n\n\n\n\nnum_classes\n: int, num_classes of the network\n\nReturns\n\n\n\n\n\n\naccuracy\n\n\n\n\n\n\nReturns\n\n\n\naccuracy\n\n\n\n\nRetruns one hot vector\n\n\ntefla.core.metrics.one_hot\n  (vec,  m=None)\n\n\nArgs\n\n\n\n\n\nvec\n: a vector\n\n\nm\n: num_classes", 
            "title": "Metrics"
        }, 
        {
            "location": "/core/metrics/#computes-accuracy-metric", 
            "text": "tefla.core.metrics.accuracy_op   (predictions,  targets,  num_classes=5)", 
            "title": "Computes accuracy metric"
        }, 
        {
            "location": "/core/metrics/#retruns-one-hot-vector", 
            "text": "tefla.core.metrics.one_hot   (vec,  m=None)", 
            "title": "Retruns one hot vector"
        }, 
        {
            "location": "/core/initializers/", 
            "text": "He Normal initializer\n\n\ntefla.core.initializers.he_normal\n  (seed=None,  scale=1.0,  dtype=tf.float32)\n\nKaiming He et al. (2015): Delving deep into rectifiers: Surpassing human-level \nperformance on imagenet classification. arXiv preprint arXiv:1502.01852.\n\n\nArgs\n\n\n\n\n\nscale\n: float\n   Scaling factor for the weights. Set this to \n1.0\n for linear and\n   sigmoid units, to \nsqrt(2)\n for rectified linear units, and\n   to \nsqrt(2/(1+alpha**2))\n for leaky rectified linear units with\n   leakiness \nalpha\n. Other transfer functions may need different factors.\n\n\n\n\n\n\nHe Uniform initializer\n\n\ntefla.core.initializers.he_uniform\n  (seed=None,  scale=1.0,  dtype=tf.float32)\n\n\nArgs\n\n\n\n\n\nscale\n: float\n   Scaling factor for the weights. Set this to \n1.0\n for linear and\n   sigmoid units, to \nsqrt(2)\n for rectified linear units, and\n   to \nsqrt(2/(1+alpha**2))\n for leaky rectified linear units with\n   leakiness \nalpha\n. Other transfer functions may need different factors.", 
            "title": "Initializations"
        }, 
        {
            "location": "/core/initializers/#he-normal-initializer", 
            "text": "tefla.core.initializers.he_normal   (seed=None,  scale=1.0,  dtype=tf.float32) \nKaiming He et al. (2015): Delving deep into rectifiers: Surpassing human-level \nperformance on imagenet classification. arXiv preprint arXiv:1502.01852.", 
            "title": "He Normal initializer"
        }, 
        {
            "location": "/core/initializers/#he-uniform-initializer", 
            "text": "tefla.core.initializers.he_uniform   (seed=None,  scale=1.0,  dtype=tf.float32)", 
            "title": "He Uniform initializer"
        }, 
        {
            "location": "/core/losses/", 
            "text": "Define a log loss\n\n\ntefla.core.losses.log_loss_custom\n  (predictions,  labels,  eps=1e-07,  name='log')\n\n\nArgs\n\n\n\n\npredictions\n: 2D tensor or array, [batch_size, num_classes] predictions of the network .\n\n\nlabels\n: 2D or array tensor, [batch_size, num_classes]  ground truth labels or target labels.\n\n\neps\n: a constant to set upper or lower limit for labels, smoothening factor\n\n\n\n\nname\n: Optional scope/name for op_scope.\n\nReturns\n\n\n\n\n\n\nA tensor with the log loss.\n\n\n\n\n\n\nReturns\n\n\n\nA tensor with the log loss.\n\n\n\n\nDefine a kappa loss, Its a continuous differentiable approximation of discrete kappa loss\n\n\ntefla.core.losses.kappa_loss\n  (predictions,  labels,  y_pow=1,  eps=1e-15,  num_ratings=5,  batch_size=32,  name='kappa')\n\n\nArgs\n\n\n\n\npredictions\n: 2D tensor or array, [batch_size, num_classes] predictions of the network .\n\n\nlabels\n: 2D tensor or array,[batch_size, num_classes]  ground truth labels or target labels.\n\n\ny_pow\n: int, to whcih the labels should be raised; useful if model diverge. e.g. y_pow=2\n\n\nnum_ratings\n: numbers of rater to used, typically num_classes of the model\n\n\nbatch_size\n: batch_size of the training or validation ops\n\n\neps\n: a float, prevents divide by zero \n\n\n\n\nname\n: Optional scope/name for op_scope.\n\nReturns\n\n\n\n\n\n\nA tensor with the kappa loss.\n\n\n\n\n\n\nReturns\n\n\n\nA tensor with the kappa loss.\n\n\n\n\nDefine a joint kappa and log loss, Kappa is a continuous differentiable approximation of discrete kappa loss\n\n\ntefla.core.losses.kappa_log_loss\n  (predictions,  labels,  label_smoothing=0.0,  y_pow=1,  batch_size=32,  log_scale=0.5,  log_offset=0.5,  name='kappa_log')\n\n\nArgs\n\n\n\n\npredictions\n: 2D tensor or array, [batch_size, num_classes] predictions of the network .\n\n\nlabels\n: 2D tensor or array,[batch_size, num_classes]  ground truth labels or target labels.\n\n\nlabel_smoothing\n: a float, used to smooth the labels for better generalization if greater than 0 then smooth the labels.\n\n\ny_pow\n: int, to whcih the labels should be raised; useful if model diverge. e.g. y_pow=2\n\n\nnum_ratings\n: numbers of rater to used, typically num_classes of the model\n\n\nbatch_size\n: batch_size of the training or validation ops\n\n\nlog_scale\n: a float, used to multiply the clipped log loss, e.g: 0.5\n\n\nlog_offset\n:a float minimum log loss offset to substract from original log loss; e.g. 0.50\n\n\n\n\nname\n: Optional scope/name for op_scope.\n\nReturns\n\n\n\n\n\n\nA tensor with the kappa log loss.\n\n\n\n\n\n\nReturns\n\n\n\nA tensor with the kappa log loss.\n\n\n\n\nDefine a joint kappa and log loss; log loss is clipped by a defined min value; Kappa is a continuous differentiable approximation of discrete kappa loss\n\n\ntefla.core.losses.kappa_log_loss_clipped\n  (predictions,  labels,  label_smoothing=0.0,  y_pow=1,  batch_size=32,  log_scale=0.5,  log_cutoff=0.8,  name='kappa_log_clipped')\n\n\nArgs\n\n\n\n\npredictions\n: 2D tensor or array, [batch_size, num_classes] predictions of the network .\n\n\nlabels\n: 2D tensor or array,[batch_size, num_classes]  ground truth labels or target labels.\n\n\nlabel_smoothing\n: a float, used to smooth the labels for better generalization if greater than 0 then smooth the labels.\n\n\ny_pow\n: int, to whcih the labels should be raised; useful if model diverge. e.g. y_pow=2\n\n\nnum_ratings\n: numbers of rater to used, typically num_classes of the model\n\n\nbatch_size\n: batch_size of the training or validation ops\n\n\nlog_scale\n: a float, used to multiply the clipped log loss, e.g: 0.5\n\n\nlog_cutoff\n:a float, minimum log loss value; e.g. 0.50\n\n\n\n\nname\n: Optional scope/name for op_scope.\n\nReturns\n\n\n\n\n\n\nA tensor with the clipped kappa log loss.\n\n\n\n\n\n\nReturns\n\n\n\nA tensor with the clipped kappa log loss.\n\n\n\n\nDefine a cross entropy loss with label smoothing\n\n\ntefla.core.losses.cross_entropy_loss\n  (logits,  labels,  label_smoothing=0.0,  weight=1.0,  name='cross_entropy_loss')\n\n\nArgs\n\n\n\n\npredictions\n: 2D tensor or array, [batch_size, num_classes] predictions of the network .\n\n\nlabels\n: 2D tensor or array,[batch_size, num_classes]  ground truth labels or target labels.\n\n\nlabel_smoothing\n: a float, used to smooth the labels for better generalizationif greater than 0 then smooth the labels.\n\n\nweight\n: scale the loss by this factor.\n\n\n\n\nname\n: Optional scope/name for op_scope.\n\nReturns\n\n\n\n\n\n\nA tensor with the cross entropy loss.\n\n\n\n\n\n\nReturns\n\n\n\nA tensor with the cross entropy loss.\n\n\n\n\nDefine a L2Loss, useful for regularize, i.e. weight decay\n\n\ntefla.core.losses.l1_l2_regularizer\n  (var,  weight_l1=1.0,  weight_l2=1.0,  name='l1_l2_regularizer')\n\n\nArgs\n\n\n\n\nvar\n: tensor to regularize.\n\n\nweight_l1\n: an optional weight to modulate the l1 loss.\n\n\nweight_l2\n: an optional weight to modulate the l2 loss.\n\n\n\n\nname\n: Optional scope/name for op_scope.\n\nReturns\n\n\n\n\n\n\nthe l1+L2 loss op.\n\n\n\n\n\n\nReturns\n\n\n\nthe l1+L2 loss op.", 
            "title": "Losses"
        }, 
        {
            "location": "/core/losses/#define-a-log-loss", 
            "text": "tefla.core.losses.log_loss_custom   (predictions,  labels,  eps=1e-07,  name='log')", 
            "title": "Define a log loss"
        }, 
        {
            "location": "/core/losses/#define-a-kappa-loss-its-a-continuous-differentiable-approximation-of-discrete-kappa-loss", 
            "text": "tefla.core.losses.kappa_loss   (predictions,  labels,  y_pow=1,  eps=1e-15,  num_ratings=5,  batch_size=32,  name='kappa')", 
            "title": "Define a kappa loss, Its a continuous differentiable approximation of discrete kappa loss"
        }, 
        {
            "location": "/core/losses/#define-a-joint-kappa-and-log-loss-kappa-is-a-continuous-differentiable-approximation-of-discrete-kappa-loss", 
            "text": "tefla.core.losses.kappa_log_loss   (predictions,  labels,  label_smoothing=0.0,  y_pow=1,  batch_size=32,  log_scale=0.5,  log_offset=0.5,  name='kappa_log')", 
            "title": "Define a joint kappa and log loss, Kappa is a continuous differentiable approximation of discrete kappa loss"
        }, 
        {
            "location": "/core/losses/#define-a-joint-kappa-and-log-loss-log-loss-is-clipped-by-a-defined-min-value-kappa-is-a-continuous-differentiable-approximation-of-discrete-kappa-loss", 
            "text": "tefla.core.losses.kappa_log_loss_clipped   (predictions,  labels,  label_smoothing=0.0,  y_pow=1,  batch_size=32,  log_scale=0.5,  log_cutoff=0.8,  name='kappa_log_clipped')", 
            "title": "Define a joint kappa and log loss; log loss is clipped by a defined min value; Kappa is a continuous differentiable approximation of discrete kappa loss"
        }, 
        {
            "location": "/core/losses/#define-a-cross-entropy-loss-with-label-smoothing", 
            "text": "tefla.core.losses.cross_entropy_loss   (logits,  labels,  label_smoothing=0.0,  weight=1.0,  name='cross_entropy_loss')", 
            "title": "Define a cross entropy loss with label smoothing"
        }, 
        {
            "location": "/core/losses/#define-a-l2loss-useful-for-regularize-ie-weight-decay", 
            "text": "tefla.core.losses.l1_l2_regularizer   (var,  weight_l1=1.0,  weight_l2=1.0,  name='l1_l2_regularizer')", 
            "title": "Define a L2Loss, useful for regularize, i.e. weight decay"
        }, 
        {
            "location": "/core/summary/", 
            "text": "Add summary to a tensor, scalar summary if the tensor is 1D, else scalar and histogram summary\n\n\ntefla.core.summary.summary_metric\n  (tensor,  name=None,  collections=None)\n\n\nArgs\n\n\n\n\n\ntensor\n: a tensor to add summary\n\n\nname\n: name of the tensor\n\n\ncollections\n: training or validation collections\n\n\n\n\n\n\nAdd summary to a tensor, scalar summary if the tensor is 1D, else  scalar and histogram summary\n\n\ntefla.core.summary.summary_activation\n  (tensor,  name=None,  collections=None)\n\n\nArgs\n\n\n\n\n\ntensor\n: a tensor to add summary\n\n\nname\n: name of the tensor\n\n\ncollections\n: training or validation collections\n\n\n\n\n\n\ncreates the summar writter for training and validation\n\n\ntefla.core.summary.create_summary_writer\n  (summary_dir,  sess)\n\n\nArgs\n\n\n\n\n\nsummary_dir\n: the directory to write summary\n\n\nsess\n: the session to sun the ops\n\n\n\n\nReturns\n\n\n\ntraining and vaidation summary writter\n\n\n\n\nAdd summary as per the ops mentioned\n\n\ntefla.core.summary.summary_param\n  (op,  tensor,  ndims,  name,  collections=None)\n\n\nArgs\n\n\n\n\n\nop\n: name of the summary op; e.g. 'stddev'\navailable ops: ['scalar', 'histogram', 'sparsity', 'mean', 'rms', 'stddev', 'norm', 'max', 'min']\n\n\ntensor\n: the tensor to add summary\n\n\nndims\n: dimension of the tensor\n\n\nname\n: name of the op\n\n\ncollections\n: training or validation collections\n\n\n\n\n\n\nAdd summary to all trainable tensors\n\n\ntefla.core.summary.summary_trainable_params\n  (summary_types,  collections=None)\n\n\nArgs\n\n\n\n\n\nsummary_type\n: a list of all sumary types to add\ne.g.: ['scalar', 'histogram', 'sparsity', 'mean', 'rms', 'stddev', 'norm', 'max', 'min']\n\n\ncollections\n: training or validation collections\n\n\n\n\n\n\nAdd summary to all gradient tensors\n\n\ntefla.core.summary.summary_gradients\n  (grad_vars,  summary_types,  collections=None)\n\n\nArgs\n\n\n\n\n\ngrads_vars\n: grads and vars list\n\n\nsummary_type\n: a list of all sumary types to add\ne.g.: ['scalar', 'histogram', 'sparsity', 'mean', 'rms', 'stddev', 'norm', 'max', 'min']\n\n\ncollections\n: training or validation collections\n\n\n\n\n\n\nAdd image summary to a image tensor\n\n\ntefla.core.summary.summary_image\n  (tensor,  name=None,  max_images=10,  collections=None)\n\n\nArgs\n\n\n\n\n\ntensor\n: a tensor to add summary\n\n\nname\n: name of the tensor\n\n\nmax_images\n: num of images to add summary\n\n\ncollections\n: training or validation collections", 
            "title": "Summaries"
        }, 
        {
            "location": "/core/summary/#add-summary-to-a-tensor-scalar-summary-if-the-tensor-is-1d-else-scalar-and-histogram-summary", 
            "text": "tefla.core.summary.summary_metric   (tensor,  name=None,  collections=None)", 
            "title": "Add summary to a tensor, scalar summary if the tensor is 1D, else scalar and histogram summary"
        }, 
        {
            "location": "/core/summary/#add-summary-to-a-tensor-scalar-summary-if-the-tensor-is-1d-else-scalar-and-histogram-summary_1", 
            "text": "tefla.core.summary.summary_activation   (tensor,  name=None,  collections=None)", 
            "title": "Add summary to a tensor, scalar summary if the tensor is 1D, else  scalar and histogram summary"
        }, 
        {
            "location": "/core/summary/#creates-the-summar-writter-for-training-and-validation", 
            "text": "tefla.core.summary.create_summary_writer   (summary_dir,  sess)", 
            "title": "creates the summar writter for training and validation"
        }, 
        {
            "location": "/core/summary/#add-summary-as-per-the-ops-mentioned", 
            "text": "tefla.core.summary.summary_param   (op,  tensor,  ndims,  name,  collections=None)", 
            "title": "Add summary as per the ops mentioned"
        }, 
        {
            "location": "/core/summary/#add-summary-to-all-trainable-tensors", 
            "text": "tefla.core.summary.summary_trainable_params   (summary_types,  collections=None)", 
            "title": "Add summary to all trainable tensors"
        }, 
        {
            "location": "/core/summary/#add-summary-to-all-gradient-tensors", 
            "text": "tefla.core.summary.summary_gradients   (grad_vars,  summary_types,  collections=None)", 
            "title": "Add summary to all gradient tensors"
        }, 
        {
            "location": "/core/summary/#add-image-summary-to-a-image-tensor", 
            "text": "tefla.core.summary.summary_image   (tensor,  name=None,  max_images=10,  collections=None)", 
            "title": "Add image summary to a image tensor"
        }, 
        {
            "location": "/da/data/", 
            "text": "", 
            "title": "Data Augmentation"
        }, 
        {
            "location": "/dataset/image_to_tfrecords/", 
            "text": "", 
            "title": "TfRecords"
        }, 
        {
            "location": "/core/training/", 
            "text": "Supervised Trainer class\n\n\ntefla.core.training.SupervisedTrainer\n  (model,  cnf,  training_iterator=\n,  validation_iterator=\n,  start_epoch=1,  resume_lr=0.01,  classification=True,  clip_norm=True,  n_iters_per_epoch=1094,  gpu_memory_fraction=0.94,  is_summary=False)\n\n\nArgs\n\n\n\n\n\nmodel\n: model definition \n\n\ncnf\n: dict, training configs\n\n\ntraining_iterator\n: iterator to use for training data access, processing and augmentations\n\n\nvalidation_iterator\n: iterator to use for validation data access, processing and augmentations\n\n\nstart_epoch\n: int, training start epoch; for resuming training provide the last \n\n\nepoch number to resume training from, its a required parameter for training data balancing\n\n\nresume_lr\n: float, learning rate to use for new training\n\n\nclassification\n: bool, classificattion or regression\n\n\nclip_norm\n: bool, to clip gradient using gradient norm, stabilizes the training\n\n\nn_iters_per_epoch\n: int,  number of iteratiosn for each epoch; \ne.g: total_training_samples/batch_size\n\n\ngpu_memory_fraction\n: amount of gpu memory to use\n\n\nis_summary\n: bool, to write summary or not\n\n\n\n\nMethods\n\n\n\n \n\n\nfit\n  (data_set,  weights_from=None,  start_epoch=1,  summary_every=10,  verbose=0)\n\n\nArgs\n\n\n\n\n\ndata_set\n: dataset instance to use to access data for training/validation\n\n\nweights_from\n: str, if not None, initializes model from exisiting weights\n\n\nstart_epoch\n: int,  epoch number to start training from\ne.g. for retarining set the epoch number you want to resume training from\n\n\nsummary_every\n: int, epoch interval to write summary; higher value means lower frequency\nof summary writing\n\n\nverbose\n: log level\n\n\n\n\n\n\nClips the gradients by the given value\n\n\ntefla.core.training.clip_grad_global_norms\n  (tvars,  loss,  opt,  global_norm=1,  gate_gradients=1,  gradient_noise_scale=4.0,  GATE_GRAPH=2,  grad_loss=None,  agre_method=None,  col_grad_ops=False)\n\n\nArgs\n\n\n\n\n\ntvars\n: trainable variables used for gradint updates\n\n\nloss\n: total loss of the network\n\n\nopt\n: optimizer\n\n\nglobal_norm\n: the maximum global norm\n\n\n\n\nReturns\n\n\n\nA list of clipped gradient to variable pairs.\n\n\n\n\nMultiply specified gradients\n\n\ntefla.core.training.multiply_gradients\n  (grads_and_vars,  gradient_multipliers)\n\n\nArgs\n\n\n\n\n\ngrads_and_vars\n: A list of gradient to variable pairs (tuples).\n\n\ngradient_multipliers\n: A map from either \nVariables\n or \nVariable\n op names\n\n\nto the coefficient by which the associated gradient should be scaled.\n\n\n\n\nReturns\n\n\n\nThe updated list of gradient to variable pairs.\n\n\n\n\nAdds scaled noise from a 0-mean normal distribution to gradients\n\n\ntefla.core.training.add_scaled_noise_to_gradients\n  (grads_and_vars,  gradient_noise_scale=10.0)\n\n\nArgs\n\n\n\n\n\ngrads_and_vars\n: list of gradient and variables\n\n\ngardient_noise_scale\n: value of noise factor\n\n\n\n\nReturns\n\n\n\nnoise added gradients", 
            "title": "Trainer"
        }, 
        {
            "location": "/core/training/#supervised-trainer-class", 
            "text": "tefla.core.training.SupervisedTrainer   (model,  cnf,  training_iterator= ,  validation_iterator= ,  start_epoch=1,  resume_lr=0.01,  classification=True,  clip_norm=True,  n_iters_per_epoch=1094,  gpu_memory_fraction=0.94,  is_summary=False)", 
            "title": "Supervised Trainer class"
        }, 
        {
            "location": "/core/training/#clips-the-gradients-by-the-given-value", 
            "text": "tefla.core.training.clip_grad_global_norms   (tvars,  loss,  opt,  global_norm=1,  gate_gradients=1,  gradient_noise_scale=4.0,  GATE_GRAPH=2,  grad_loss=None,  agre_method=None,  col_grad_ops=False)", 
            "title": "Clips the gradients by the given value"
        }, 
        {
            "location": "/core/training/#multiply-specified-gradients", 
            "text": "tefla.core.training.multiply_gradients   (grads_and_vars,  gradient_multipliers)", 
            "title": "Multiply specified gradients"
        }, 
        {
            "location": "/core/training/#adds-scaled-noise-from-a-0-mean-normal-distribution-to-gradients", 
            "text": "tefla.core.training.add_scaled_noise_to_gradients   (grads_and_vars,  gradient_noise_scale=10.0)", 
            "title": "Adds scaled noise from a 0-mean normal distribution to gradients"
        }
    ]
}